{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "  \\frac{P({Z, K, X})}{Q({Z|X})} = \n",
    "  P({K}) \n",
    "  P \\left({X| Z_{\\mathrm{pa}{X}}^{K_{\\mathrm{pa}{X}}}} \\right)  \n",
    "  \\prod_i \n",
    "  \\frac{P\\left({Z_i^{K_i}| Z^{K_{\\mathrm{pa}(i)}}_{\\mathrm{pa}(i)}} \\right)}\n",
    "  {Q \\left(  \n",
    "    Z_i^{K_i}| Z^{K_i}_{\\mathrm{qa}(i)}\n",
    "  \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The computation\n",
    "\n",
    "1. Form joint P/Q: index prior * lik * product of latent P/Qs\n",
    "2. $\\mathcal{L}$: sum out K, then log P - log Q, then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp\n",
    "from tvi import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## First with no plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu, var : WrappedDist(Normal, mu, var)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "# Prior\n",
    "# a -> b -> c observed\n",
    "def chain_dist(trace, n=3):\n",
    "    a = trace[\"a\"].Normal(t.ones(n) * TRUE_MEAN_A, scale)\n",
    "    b = trace[\"b\"].Normal(a, scale)\n",
    "    c = trace[\"c\"].Normal(b, scale)\n",
    "    \n",
    "    return c\n",
    "\n",
    "# def chain_dist(trace, n=3):\n",
    "#     a = trace[\"a\"](Norm(t.ones(n) * TRUE_MEAN_A, scale))\n",
    "#     b = trace[\"b\"](Norm(a, scale))\n",
    "#     c = trace[\"c\"](Norm(b, scale))\n",
    "    \n",
    "#     return c\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.ones(n))\n",
    "        self.mean_b = nn.Parameter(t.ones(n))\n",
    "        self.logscale_a = nn.Parameter(t.ones(n)) # t.log(t.ones(n))\n",
    "        self.logscale_b = nn.Parameter(t.ones(n))\n",
    "    \n",
    "    # TODO: make this actually depend on the params\n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"](Norm(self.mean_a, t.exp(self.logscale_a)))\n",
    "        b = trace[\"b\"](Norm(self.mean_b, t.exp(self.logscale_b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "            1. s = sample Q\n",
    "            2. lp_Q = eval Q.logprob(s)\n",
    "            3. lp_P = eval P.logprob(s)\n",
    "            4. f = lp_P - lp_Q\n",
    "            5. loss = combine fs\n",
    "        \"\"\"\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected) #, data={\"__c\": self.data}\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs \n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        sum_out_pos(eval_trace)\n",
    "        sum_out_pos(sample_trace)\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        \n",
    "        # reduce gives loss\n",
    "        loss_dict = tpp.combine_tensors(tensors)\n",
    "\n",
    "        return tpp.undict(loss_dict)\n",
    "\n",
    "\n",
    "def setup_and_run(tvi, ep=2000, eta=1) :\n",
    "    optimiser = t.optim.Adam(tvi.q.parameters(), lr=eta) # optimising q only    \n",
    "    optimise(tvi, optimiser, ep)\n",
    "    \n",
    "    return tvi\n",
    "\n",
    "\n",
    "def optimise(tvi, optimiser, eps) :\n",
    "    for i in range(eps):\n",
    "        optimiser.zero_grad()\n",
    "        loss = - tvi() \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "\n",
    "def sample_generator(nProtected, P, dataName=\"__c\") :\n",
    "    k = 1\n",
    "    trp = sampler(k, nProtected)\n",
    "    P(trp)\n",
    "    return trp.trace.out_dicts[\"sample\"][dataName] \\\n",
    "            .squeeze(0)\n",
    "        \n",
    "\n",
    "def get_error_on_a(a_mean, n, tvi) :\n",
    "    a_mean = t.ones(n) * a_mean\n",
    "    return a_mean - tvi.q.mean_a\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = chain_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sampler() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-793bd3a9febf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;31m#5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRUE_MEAN_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merror_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_error\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTRUE_MEAN_A\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-86c48e2f37cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(nvars, nProtected, k, epochs, true_mean, lr)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Get _c data by sampling generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnProtected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"__c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnProtected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-86c48e2f37cc>\u001b[0m in \u001b[0;36msample_generator\u001b[0;34m(nProtected, P, dataName)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnProtected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"__c\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnProtected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sampler() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "ep = 20 #5000 \n",
    "error, tvi = main(nvars=3, k=2, epochs=ep, true_mean=TRUE_MEAN_A, lr=0.1)\n",
    "\n",
    "mean_error = error.abs().sum() / 3 \n",
    "error_percent = mean_error / TRUE_MEAN_A * 100\n",
    "print(f\"Mean error on the parameter's of A: {error_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI, No plates but including deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def chain_dist_del(trace):\n",
    "#     a = trace[\"a\"](Norm(t.ones(n)))\n",
    "#     b = trace[\"b\"](Norm(a))\n",
    "#     c = trace[\"c\"](Norm(b))\n",
    "#     (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "#     d = trace[\"d\"](Norm(c))\n",
    "    \n",
    "#     return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # call sampler on Q. \n",
    "# # gives you the samples and a log Q tensor `log_prob`\n",
    "# tr1 = sampler(draws, nProtected, data=data)\n",
    "\n",
    "# val = P(tr1)\n",
    "# log_q = tr1.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "# # compute the log_probs\n",
    "\n",
    "# # pass these to evaluator, which does a lookup for all the latents \n",
    "# # gives you log P for each latent\n",
    "# tr2 = evaluator(tr1, nProtected, data=data)\n",
    "# val = P(tr2)\n",
    "\n",
    "# #tr2.trace.out_dicts[\"log_prob\"]\n",
    "# #log_p = \n",
    "\n",
    "# #Q = pytorch.module\n",
    "# #    - `q.forward()` will look like chain_dist\n",
    "    \n",
    "\n",
    "# #- optimise it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plate VI\n",
    "\n",
    "- For plates, we just don't filter [@17](https://github.com/LaurenceA/tpp/blob/bd1fe20dcf86a1c02cc0424632571fba998d104f/utils.py#L17)\n",
    "- Painful stuff: need to keep the generative order (e.g. a, b, c, d)\n",
    "    - because we start by summing the lowest-level plates\n",
    "        - solution: enforce that the last variable is a leaf e.g. `d`\n",
    "- Careful when combining P & Q tensors: maintain the ordering!\n",
    "\n",
    "- Plates: doing the summation backwards through the plates, yeah?\n",
    "    - This implies tricky implementation blah\n",
    "    - Py 3.6 dicts are ordered by insertion though, so use that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu, var : WrappedDist(Normal, mu, var)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.ones(n))\n",
    "        self.mean_b = nn.Parameter(t.ones(n))\n",
    "        self.logscale_a = nn.Parameter(t.ones(n)) # t.log(t.ones(n))\n",
    "        self.logscale_b = nn.Parameter(t.ones(n))\n",
    "    \n",
    "    # TODO: make this actually depend on the params\n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"](Norm(self.mean_a, t.exp(self.logscale_a)))\n",
    "        b = trace[\"b\"](Norm(self.mean_b, t.exp(self.logscale_b)))\n",
    "\n",
    "        \n",
    "# example directed graph with plate repeats\n",
    "# 3(a) -> 4(b) -> c -> d\n",
    "def plate_dist(trace, n=3):\n",
    "    Na = Norm(t.ones(n) * TRUE_MEAN_A, 1)\n",
    "    a = trace[\"a\"](Na, plate_name=\"A\", plate_shape=3)\n",
    "    Nb = Norm(a, 1)\n",
    "    b = trace[\"b\"](Nb, plate_name=\"B\", plate_shape=4)\n",
    "    Nc = Norm(b, 1)\n",
    "    c = trace[\"c\"](Nc)\n",
    "    \n",
    "    #(c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    #Nd = Norm(c, 1)\n",
    "    #d = trace[\"d\"](Nd)\n",
    "    \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-pass example\n",
    "k = 2\n",
    "nProtected = 2\n",
    "p = plate_dist\n",
    "q = ChainQ()\n",
    "\n",
    "x = sample_generator(nProtected, p, dataName=\"__c\")\n",
    "tr = sample_and_eval(plate_dist, draws=2, nProtected=2)\n",
    "\n",
    "# sample_trace = sampler(k, nProtected, data={\"__c\": x})\n",
    "# q.sample(sample_trace)\n",
    "# eval_trace = evaluator(sample_trace, nProtected, data={\"__c\": x})\n",
    "# # compute P logprobs \n",
    "# p(eval_trace)\n",
    "\n",
    "sum_out_pos(tr)\n",
    "lps = tr.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "tpp.combine_over_plates(lps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sample_trace = sampler(k, nProtected, data={\"__c\": x})\n",
    "# sample recognition model Q -> Q-sample and Q-logprobs\n",
    "q.sample(sample_trace)\n",
    "\n",
    "eval_trace = evaluator(sample_trace, nProtected, data={\"__c\": x})\n",
    "# compute P logprobs \n",
    "p(eval_trace)\n",
    "\n",
    "sum_out_pos(sample_trace)\n",
    "sum_out_pos(eval_trace)\n",
    "lps = tr.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "tpp.combine_over_plates(lps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with new combine func\n",
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "            1. s = sample Q\n",
    "            2. lp_Q = eval Q.logprob(s)\n",
    "            3. lp_P = eval P.logprob(s)\n",
    "            4. f = lp_P - lp_Q\n",
    "            5. loss = combine fs\n",
    "        \"\"\"\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected, data={\"__c\": self.data})\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs \n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        sum_out_pos(eval_trace)\n",
    "        sum_out_pos(sample_trace)\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        \n",
    "        # reduce gives loss\n",
    "        loss_dict = combine_over_plates(tensors)\n",
    "\n",
    "        return tpp.undict(loss_dict)\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = plate_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 20 #5000 \n",
    "error, tvi = main(nvars=3, k=2, epochs=ep, true_mean=TRUE_MEAN_A, lr=0.1)\n",
    "\n",
    "mean_error = error.abs().sum() / 3 \n",
    "error_percent = mean_error / TRUE_MEAN_A * 100\n",
    "print(f\"Mean error on the parameter's of A: {error_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alt frontend\n",
    "\n",
    "- PLATES\n",
    "    - LogProbK -> log_probs will have the right order but nothing else will\n",
    "    - prior component K -> K_a\n",
    "    - remove K from all -> replace with k_a, k_b\n",
    "- simple_trace\n",
    "    - convention: sample K first, \n",
    "    - plates go up-and-left as we go deeper\n",
    "    - \n",
    "- get a few examples of models\n",
    "    - \"we only need enough variables for a neurips paper\"\n",
    "- hopefully able to handle the pos dims generically in the prior\n",
    "- Also want some checking code: run through Ps & lps to see if its sane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(tr): \n",
    "    tr.set_names('a', 'b')\n",
    "    tr['a'] = Normal(tr.zeros(()), 1)\n",
    "    tr['b'] = Normal(tr['a'], 1)\n",
    "    tr.add_remove_names(add_names=('c',), remove_names=('a',))\n",
    "    tr['c'] = Normal(tr['b'], 1, sample_shape=3, sample_names='plate_a')\n",
    "    print(tr['c'].names)\n",
    "    print(tr['c'].shape)\n",
    "    tr['obs'] = Normal(tr['c'], 1, sample_shape=5, sample_names='plate_b')\n",
    "\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m_a = nn.Parameter(t.zeros(()))\n",
    "        self.m_b = nn.Parameter(t.zeros(()))\n",
    "        self.m_c = nn.Parameter(t.zeros((3,), names=('plate_a',)))\n",
    "    \n",
    "    def forward(self, tr):\n",
    "        tr['a'] = Normal(tr.pad(self.m_a), 1)\n",
    "        tr['b'] = Normal(tr.pad(self.m_b), 1)\n",
    "        tr['c'] = Normal(tr.pad(self.m_c), 1)\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
