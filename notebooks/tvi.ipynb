{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "  \\frac{P({Z, K, X})}{Q({Z|X})} = \n",
    "  P({K}) \n",
    "  P \\left({X| Z_{\\mathrm{pa}{X}}^{K_{\\mathrm{pa}{X}}}} \\right)  \n",
    "  \\prod_i \n",
    "  \\frac{P\\left({Z_i^{K_i}| Z^{K_{\\mathrm{pa}(i)}}_{\\mathrm{pa}(i)}} \\right)}\n",
    "  {Q \\left(  \n",
    "    Z_i^{K_i}| Z^{K_i}_{\\mathrm{qa}(i)}\n",
    "  \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The computation\n",
    "\n",
    "1. Form joint P/Q: index prior * lik * product of latent P/Qs\n",
    "2. $\\mathcal{L}$: sum out K, then log P - log Q, then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp\n",
    "from tvi import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## First with no plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu, var : WrappedDist(Normal, mu, var)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "# Prior\n",
    "# a -> b -> c observed\n",
    "def chain_dist(trace, n=3):\n",
    "    a = trace[\"a\"].Normal(t.ones(n) * TRUE_MEAN_A, scale)\n",
    "    b = trace[\"b\"].Normal(a, scale)\n",
    "    c = trace[\"c\"].Normal(b, scale)\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.ones(n))\n",
    "        self.mean_b = nn.Parameter(t.ones(n))\n",
    "        self.logscale_a = nn.Parameter(t.ones(n)) # t.log(t.ones(n))\n",
    "        self.logscale_b = nn.Parameter(t.ones(n))\n",
    "    \n",
    "    # TODO: make this actually depend on the params\n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"].Normal(self.mean_a, t.exp(self.logscale_a))\n",
    "        b = trace[\"b\"].Normal(self.mean_b, t.exp(self.logscale_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "            1. s = sample Q\n",
    "            2. lp_Q = eval Q.logprob(s)\n",
    "            3. lp_P = eval P.logprob(s)\n",
    "            4. f = lp_P - lp_Q\n",
    "            5. loss = combine fs\n",
    "        \"\"\"\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected) #, data={\"__c\": self.data}\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs \n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        sum_out_pos(eval_trace)\n",
    "        sum_out_pos(sample_trace)\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        \n",
    "        # reduce gives loss\n",
    "        loss_dict = tpp.combine_tensors(tensors)\n",
    "\n",
    "        return tpp.undict(loss_dict)\n",
    "\n",
    "\n",
    "def setup_and_run(tvi, ep=2000, eta=1) :\n",
    "    optimiser = t.optim.Adam(tvi.q.parameters(), lr=eta) # optimising q only    \n",
    "    optimise(tvi, optimiser, ep)\n",
    "    \n",
    "    return tvi\n",
    "\n",
    "\n",
    "def optimise(tvi, optimiser, eps) :\n",
    "    for i in range(eps):\n",
    "        optimiser.zero_grad()\n",
    "        loss = - tvi() \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "\n",
    "def sample_generator(nProtected, P, dataName=\"__c\") :\n",
    "    k = 1\n",
    "    trp = sampler(k, nProtected)\n",
    "    P(trp)\n",
    "    return trp.trace.out_dicts[\"sample\"][dataName] \\\n",
    "            .squeeze(0)\n",
    "        \n",
    "\n",
    "def get_error_on_a(a_mean, n, tvi) :\n",
    "    a_mean = t.ones(n) * a_mean\n",
    "    return a_mean - tvi.q.mean_a\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = chain_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': {'__a': tensor([[[ 6.4433,  9.0297, 10.5075]],\n",
       "  \n",
       "          [[12.4035, 10.4682,  4.3390]],\n",
       "  \n",
       "          [[10.4899,  7.4070,  8.6449]],\n",
       "  \n",
       "          [[ 9.8458,  9.2511,  7.2392]]], names=('_k__a', None, None)),\n",
       "  '__b': tensor([[[[ 5.0056,  4.1001,  9.1492]]],\n",
       "  \n",
       "  \n",
       "          [[[ 8.7417, 19.9388,  1.9296]]],\n",
       "  \n",
       "  \n",
       "          [[[ 7.5112,  3.7101,  9.2609]]],\n",
       "  \n",
       "  \n",
       "          [[[ 5.4997,  3.9223,  5.0610]]]], names=('_k__b', '_k__a', None, None)),\n",
       "  '__c': tensor([[[[[ 5.1926,  3.4912, 10.6923]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 7.3232, 21.7457,  0.8533]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 5.4600,  5.1678, 11.6522]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 4.7056,  3.8290,  7.8642]]]]],\n",
       "         names=('_k__c', '_k__b', '_k__a', None, None))},\n",
       " 'log_prob': {'__a': tensor([[[-2.3506, -2.1443, -2.2651]],\n",
       "  \n",
       "          [[-2.6814, -2.2596, -2.9556]],\n",
       "  \n",
       "          [[-2.2626, -2.2107, -2.1413]],\n",
       "  \n",
       "          [[-2.1895, -2.1514, -2.2295]]], names=('_k__a', None, None)),\n",
       "  '__b': tensor([[[[ -1.7383,  -2.7746,  -2.1905]],\n",
       "  \n",
       "           [[ -3.2999,  -3.1845,  -8.2524]],\n",
       "  \n",
       "           [[ -2.8402,  -2.2563,  -2.1304]],\n",
       "  \n",
       "           [[ -2.6728,  -2.8406,  -2.4313]]],\n",
       "  \n",
       "  \n",
       "          [[[ -2.6516, -10.2840,  -4.6492]],\n",
       "  \n",
       "           [[ -2.4692,  -6.8711,  -2.0776]],\n",
       "  \n",
       "           [[ -2.1951, -18.4291,  -4.0688]],\n",
       "  \n",
       "           [[ -2.1262,  -9.6023,  -3.5443]]],\n",
       "  \n",
       "  \n",
       "          [[[ -2.0419,  -2.9502,  -2.1907]],\n",
       "  \n",
       "           [[ -2.6446,  -3.3688,  -8.5757]],\n",
       "  \n",
       "           [[ -2.2870,  -2.4122,  -2.1501]],\n",
       "  \n",
       "           [[ -2.1737,  -3.0180,  -2.4819]]],\n",
       "  \n",
       "  \n",
       "          [[[ -1.7030,  -2.8516,  -2.8263]],\n",
       "  \n",
       "           [[ -3.1331,  -3.2657,  -1.6481]],\n",
       "  \n",
       "           [[ -2.6874,  -2.3238,  -2.3298]],\n",
       "  \n",
       "           [[ -2.5274,  -2.9185,  -1.9385]]]],\n",
       "         names=('_k__b', '_k__a', None, None)),\n",
       "  '__c': tensor([[[[[  -1.5549,   -1.2513,   -2.3995]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.3208,   -5.3843, -166.2850]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.9865,   -1.1673,   -2.3788]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.5627,   -1.2061,   -8.4569]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[  -2.8824, -146.1655,   -5.8605]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.0115,   -3.0178,   -1.2771]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.8953, -198.0239,   -5.8969]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.3944, -167.2559,   -4.0882]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[  -1.6360,   -1.8519,   -2.6971]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.2545,   -4.6119, -216.1706]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.9398,   -2.2521,   -2.6597]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.5991,   -2.0037,  -11.0592]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[  -1.4663,   -1.2637,   -2.0528]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.4642,   -5.2012,  -64.4466]]],\n",
       "  \n",
       "  \n",
       "           [[[  -2.0990,   -1.2486,   -2.0665]]],\n",
       "  \n",
       "  \n",
       "           [[[  -1.5443,   -1.2465,   -3.3933]]]]],\n",
       "         names=('_k__c', '_k__b', '_k__a', None, None))}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-pass example\n",
    "k = 2\n",
    "nProtected = 2\n",
    "p = chain_dist\n",
    "q = ChainQ()\n",
    "\n",
    "tr1 = trace({}, SampleLogProbK(K=4, protected_dims=nProtected))\n",
    "val = chain_dist(tr1)\n",
    "tr2 = trace({\"data\": {}, \"sample\": tr1.trace.out_dicts[\"sample\"]}, LogProbK(tr1.trace.fn.dim_names))\n",
    "val = chain_dist(tr2)\n",
    "\n",
    "tr2.trace.out_dicts\n",
    "# sample_trace = trace({}, SampleLogProbK(K=4, protected_dims=2))\n",
    "# _ = chain_dist(sample_trace)\n",
    "# eval_trace = trace({\"data\": {}, \"sample\": sample_trace.trace.out_dicts[\"sample\"]}, \\\n",
    "#                    LogProbK(sample_trace.trace.fn.dim_names))\n",
    "# _ = chain_dist(eval_trace)\n",
    "# p(eval_trace)\n",
    "\n",
    "\n",
    "# x = sample_generator(nProtected, p, dataName=\"__c\")\n",
    "# sample_trace = sampler(k, nProtected)\n",
    "# q.sample(sample_trace)\n",
    "# eval_trace = evaluator(sample_trace, nProtected, data={\"__c\": x})\n",
    "# compute P logprobs \n",
    "\n",
    "\n",
    "#sum_out_pos(tr2)\n",
    "lps = tr2.trace.out_dicts[\"log_prob\"]\n",
    " \n",
    "#tpp.combine_tensors(lps)\n",
    "\n",
    "tr2.trace.out_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ep = 20 #5000 \n",
    "error, tvi = main(nvars=3, k=2, epochs=ep, true_mean=TRUE_MEAN_A, lr=0.1)\n",
    "\n",
    "mean_error = error.abs().sum() / 3 \n",
    "error_percent = mean_error / TRUE_MEAN_A * 100\n",
    "print(f\"Mean error on the parameter's of A: {error_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI, No plates but including deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def chain_dist_del(trace):\n",
    "#     a = trace[\"a\"](Norm(t.ones(n)))\n",
    "#     b = trace[\"b\"](Norm(a))\n",
    "#     c = trace[\"c\"](Norm(b))\n",
    "#     (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "#     d = trace[\"d\"](Norm(c))\n",
    "    \n",
    "#     return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # call sampler on Q. \n",
    "# # gives you the samples and a log Q tensor `log_prob`\n",
    "# tr1 = sampler(draws, nProtected, data=data)\n",
    "\n",
    "# val = P(tr1)\n",
    "# log_q = tr1.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "# # compute the log_probs\n",
    "\n",
    "# # pass these to evaluator, which does a lookup for all the latents \n",
    "# # gives you log P for each latent\n",
    "# tr2 = evaluator(tr1, nProtected, data=data)\n",
    "# val = P(tr2)\n",
    "\n",
    "# #tr2.trace.out_dicts[\"log_prob\"]\n",
    "# #log_p = \n",
    "\n",
    "# #Q = pytorch.module\n",
    "# #    - `q.forward()` will look like chain_dist\n",
    "    \n",
    "\n",
    "# #- optimise it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plate VI\n",
    "\n",
    "- For plates, we just don't filter [@17](https://github.com/LaurenceA/tpp/blob/bd1fe20dcf86a1c02cc0424632571fba998d104f/utils.py#L17)\n",
    "- Painful stuff: need to keep the generative order (e.g. a, b, c, d)\n",
    "    - because we start by summing the lowest-level plates\n",
    "        - solution: enforce that the last variable is a leaf e.g. `d`\n",
    "- Careful when combining P & Q tensors: maintain the ordering!\n",
    "\n",
    "- Plates: doing the summation backwards through the plates, yeah?\n",
    "    - This implies tricky implementation blah\n",
    "    - Py 3.6 dicts are ordered by insertion though, so use that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu, var : WrappedDist(Normal, mu, var)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.ones(n))\n",
    "        self.mean_b = nn.Parameter(t.ones(n))\n",
    "        self.logscale_a = nn.Parameter(t.ones(n)) # t.log(t.ones(n))\n",
    "        self.logscale_b = nn.Parameter(t.ones(n))\n",
    "    \n",
    "    # TODO: make this actually depend on the params\n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"](Norm(self.mean_a, t.exp(self.logscale_a)))\n",
    "        b = trace[\"b\"](Norm(self.mean_b, t.exp(self.logscale_b)))\n",
    "\n",
    "        \n",
    "# example directed graph with plate repeats\n",
    "# 3(a) -> 4(b) -> c -> d\n",
    "def plate_dist(trace, n=3):\n",
    "    Na = Norm(t.ones(n) * TRUE_MEAN_A, 1)\n",
    "    a = trace[\"a\"](Na, plate_name=\"A\", plate_shape=3)\n",
    "    Nb = Norm(a, 1)\n",
    "    b = trace[\"b\"](Nb, plate_name=\"B\", plate_shape=4)\n",
    "    Nc = Norm(b, 1)\n",
    "    c = trace[\"c\"](Nc)\n",
    "    \n",
    "    #(c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    #Nd = Norm(c, 1)\n",
    "    #d = trace[\"d\"](Nd)\n",
    "    \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-pass example\n",
    "k = 2\n",
    "nProtected = 2\n",
    "p = plate_dist\n",
    "q = ChainQ()\n",
    "\n",
    "x = sample_generator(nProtected, p, dataName=\"__c\")\n",
    "tr = sample_and_eval(plate_dist, draws=2, nProtected=2)\n",
    "\n",
    "# sample_trace = sampler(k, nProtected, data={\"__c\": x})\n",
    "# q.sample(sample_trace)\n",
    "# eval_trace = evaluator(sample_trace, nProtected, data={\"__c\": x})\n",
    "# # compute P logprobs \n",
    "# p(eval_trace)\n",
    "\n",
    "sum_out_pos(tr)\n",
    "lps = tr.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "tpp.combine_over_plates(lps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sample_trace = sampler(k, nProtected, data={\"__c\": x})\n",
    "# sample recognition model Q -> Q-sample and Q-logprobs\n",
    "q.sample(sample_trace)\n",
    "\n",
    "eval_trace = evaluator(sample_trace, nProtected, data={\"__c\": x})\n",
    "# compute P logprobs \n",
    "p(eval_trace)\n",
    "\n",
    "sum_out_pos(sample_trace)\n",
    "sum_out_pos(eval_trace)\n",
    "lps = tr.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "tpp.combine_over_plates(lps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with new combine func\n",
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "            1. s = sample Q\n",
    "            2. lp_Q = eval Q.logprob(s)\n",
    "            3. lp_P = eval P.logprob(s)\n",
    "            4. f = lp_P - lp_Q\n",
    "            5. loss = combine fs\n",
    "        \"\"\"\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected, data={\"__c\": self.data})\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs \n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        sum_out_pos(eval_trace)\n",
    "        sum_out_pos(sample_trace)\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        \n",
    "        # reduce gives loss\n",
    "        loss_dict = combine_over_plates(tensors)\n",
    "\n",
    "        return tpp.undict(loss_dict)\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = plate_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 20 #5000 \n",
    "error, tvi = main(nvars=3, k=2, epochs=ep, true_mean=TRUE_MEAN_A, lr=0.1)\n",
    "\n",
    "mean_error = error.abs().sum() / 3 \n",
    "error_percent = mean_error / TRUE_MEAN_A * 100\n",
    "print(f\"Mean error on the parameter's of A: {error_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alt frontend\n",
    "\n",
    "- PLATES\n",
    "    - LogProbK -> log_probs will have the right order but nothing else will\n",
    "    - prior component K -> K_a\n",
    "    - remove K from all -> replace with k_a, k_b\n",
    "- simple_trace\n",
    "    - convention: sample K first, \n",
    "    - plates go up-and-left as we go deeper\n",
    "    - \n",
    "- get a few examples of models\n",
    "    - \"we only need enough variables for a neurips paper\"\n",
    "- hopefully able to handle the pos dims generically in the prior\n",
    "- Also want some checking code: run through Ps & lps to see if its sane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(tr): \n",
    "    tr.set_names('a', 'b')\n",
    "    tr['a'] = Normal(tr.zeros(()), 1)\n",
    "    tr['b'] = Normal(tr['a'], 1)\n",
    "    tr.add_remove_names(add_names=('c',), remove_names=('a',))\n",
    "    tr['c'] = Normal(tr['b'], 1, sample_shape=3, sample_names='plate_a')\n",
    "    print(tr['c'].names)\n",
    "    print(tr['c'].shape)\n",
    "    tr['obs'] = Normal(tr['c'], 1, sample_shape=5, sample_names='plate_b')\n",
    "\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m_a = nn.Parameter(t.zeros(()))\n",
    "        self.m_b = nn.Parameter(t.zeros(()))\n",
    "        self.m_c = nn.Parameter(t.zeros((3,), names=('plate_a',)))\n",
    "    \n",
    "    def forward(self, tr):\n",
    "        tr['a'] = Normal(tr.pad(self.m_a), 1)\n",
    "        tr['b'] = Normal(tr.pad(self.m_b), 1)\n",
    "        tr['c'] = Normal(tr.pad(self.m_c), 1)\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
